{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341d394-3e01-47af-8c01-fb74e030ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR COLAB ONLY ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba7f55-280f-4e41-98c1-58dd5e448b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fbcdd-9f48-42e6-9fba-7f1e8bc74f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd /content/drive/My Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849c5f5-a230-4a72-9251-a80d6adda0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fb8ac-c7f2-4b79-b90e-d52d575d8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6f3e4-56d3-48e3-8abe-75acf4de9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import musdb\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from moviepy.editor import AudioFileClip, ColorClip\n",
    "from scipy.io.wavfile import write\n",
    "from modules.model import ssnet\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b0248-4888-4426-aa8a-655185957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUILD DATASET ###\n",
    "\n",
    "# mus.audio : The mixture\n",
    "# mus.stems[0] : The mixture\n",
    "# mus.stems[1] : The drums\n",
    "# mus.stems[2] : The bass\n",
    "# mus.stems[3] : Rest of the accompaniment\n",
    "# mus.stems[4] : The vocals\n",
    "\n",
    "# #TODO: Split mus into train and test subsets: \n",
    "# --> mus_train = musdb.DB(subsets=\"train\") \n",
    "# --> mus_test = musdb.DB(subsets=\"test\")\n",
    "\n",
    "# #TODO: audio with a number of frames > 9000000 crash the notebook... find a way around this\n",
    "\n",
    "# #TODO: We want the input to be the mixture (stems[1] or mus.audio)\n",
    "# --> target has to be everything but vocals, thus: Find a way to mix stems[1], stems[2], stems[3]\n",
    "# --> temporarly here the target is stems[4] (vocals)\n",
    "\n",
    "# #Note: Soundtracks are too big to be processed all at once by the model, so here we process the soundtrack by chunks of 100000 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbd15f-f72a-4cd2-ae8b-1dfa2a6b4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = musdb.DB(root=\"data/musdb18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dab42f-d6c8-4b73-8c81-8d19f889352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "frames = 30000\n",
    "for t in tqdm(mus):\n",
    "    audio = t.stems[0]\n",
    "    audio_len = audio.shape[0]\n",
    "    if audio_len > 9000000:\n",
    "      continue\n",
    "    target = t.stems[4]\n",
    "    slices = list(range(0, audio_len, frames))\n",
    "    for i in range(0, slices[-1], frames):\n",
    "        if slices[-1] - i < frames:\n",
    "            continue\n",
    "        x = torch.tensor(audio[i:i+frames]).T.float()\n",
    "        x = x.view(1,2,-1)\n",
    "        y = torch.tensor(target[i:i+frames]).T.float()\n",
    "        y = y.view(1,2,-1)\n",
    "        dataset.append((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e5be6-6bd7-4f1b-948e-a1816311244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset, open('dataset.pkl', 'wb'))\n",
    "#dataset = pickle.load(open('dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d4e6d-edd1-42ed-a735-1aad07c6b731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6911ea0-e97f-4a60-b0c0-505bded07c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6599a8-29ec-459a-b303-376acd293090",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LR = 0.01\n",
    "WD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504688d-953d-462e-ad5b-91d82448df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssnet_ = ssnet().float()\n",
    "optimizer = optim.Adam(ssnet_.parameters(),lr=LR, weight_decay=WD)\n",
    "loss_function = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd81525-0cd5-429c-821c-c21a22b70c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssnet_.train()\n",
    "for epoch in range(EPOCHS):\n",
    "  for i in dataset:\n",
    "    x,y = i[0], i[1]\n",
    "    out = ssnet_(x)\n",
    "    ssnet_.zero_grad()\n",
    "    loss = loss_function(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033aa670-e26f-4894-8487-78a2f11f6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssnet_.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289900d-d50b-43c5-82b9-6c7386576a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e9910-f18f-4641-8f65-63888764be02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114af1e-85ab-4b88-a1a3-599ccadfb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST MODEL ###\n",
    "\n",
    "# Here we ask the trained model to predict n chunks of frames. \n",
    "# We concatenate these predictions and output them in an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67328c3e-5f14-407c-86ce-d2d91b5ef3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_track = []\n",
    "cnt = 0\n",
    "for i in dataset:\n",
    "  cnt += 1\n",
    "  x = i[0]\n",
    "  out = ssnet_(x).view(-1,2)\n",
    "  out = out.detach().numpy()\n",
    "  rebuild_track.append(out)\n",
    "  if cnt == 100:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02003491-dfe0-467b-bbe7-3c265772bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.concatenate(rebuild_track, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21541eb-95f8-498d-bed6-91a47c80a3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e9ad6-d8bd-49c2-8475-ca8bfbf34392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_rate = 44100\n",
    "#wav_file = \"output.wav\"\n",
    "#write(wav_file, sample_rate, audio_data)\n",
    "#audio_clip = AudioFileClip(wav_file)\n",
    "#video_duration = audio_clip.duration\n",
    "#video_clip = ColorClip(size=(640, 480), color=(0, 0, 0), duration=video_duration)\n",
    "#video_clip = video_clip.set_audio(audio_clip)\n",
    "#video_clip.fps = 24\n",
    "#mp4_file = \"output.mp4\"\n",
    "#video_clip.write_videofile(mp4_file, codec=\"libx264\", audio_codec=\"aac\", fps=24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
